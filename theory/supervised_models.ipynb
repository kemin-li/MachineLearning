{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from scipy import stats\n",
    "from IPython.core.display import HTML, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrimitive vs. Generative methods\n",
    "The goal is to estimate $P(y|X)$\n",
    "### generative methods\n",
    "Assume functional form of $P(X|y)$ and $P(y)$, estimate $P(X, y)$ first. First learn probability distribution of the data, then using Bayes rule.\n",
    "$$P(y|X) = \\frac{P(X,|y)P(X)}{P(X)}$$\n",
    "### discrimitive methods\n",
    "Assume functional form of $P(y|X)$, directly estimate $P(y|X)$ from training data. Learn the decision boundary\n",
    "\n",
    "Key take aways\n",
    "- The generative model does indeed have a higher asymptotic error (as the number of training examples become large) than the discriminative model. This means generally, discrimitive methods perform better than generative classifiers if we have many data. Because it make less assumption on data model. We can estimate statistics rather reliably if have enough data.\n",
    "- The generative model may approach its asymptotic error much faster than the discriminative model â€“ possibly with a number of training examples that is only logarithmic, rather than linear, in the number of parameters. This means generative models may perform better when we have small data and model assumption is corret.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common loss fuctions\n",
    "- Logistic loss: $\\log(1+\\exp(-y\\hat y))$, logistic regression\n",
    "- Hinge Loss: $\\max(0, 1-y\\hat y)$, SVM\n",
    "- Cross entropy: $-y\\log(\\hat y)-(1-y)\\log(1-\\hat y)$, Nueral network\n",
    "- Least squared error: $ (y-\\hat y)^2$, linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "- Assume $y|x;\\theta\\sim\\mathcal{N}(\\mu,\\sigma^2)$\n",
    "- Least square solver: $\\boxed{\\theta=(X^TX)^{-1}X^Ty}$\n",
    "- LMS algorithm: $\\boxed{\\forall j,\\quad \\theta_j \\leftarrow \\theta_j+\\alpha\\sum_{i=1}^m\\left[y^{(i)}-h_\\theta(x^{(i)})\\right]x_j^{(i)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "- Sigmoid function: $g(z)=\\frac{1}{1+e^{-z}}$\n",
    "- Assume $y|x;\\theta, \\rm{where}\\ \\theta\\sim\\rm{Bernoulli}(\\phi)$\n",
    "$$\\boxed{\\phi=p(y=1|x;\\theta)=\\frac{1}{1+\\exp(-\\theta^Tx)}=g(\\theta^Tx)}$$\n",
    "- Softmax for multi-class logistic regression\n",
    "$$\\displaystyle\\phi_i=\\frac{\\exp(\\theta_i^Tx)}{\\displaystyle\\sum_{j=1}^K\\exp(\\theta_j^Tx)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear models (GLM)\n",
    "Generalized linear models aim at predicting a random variable $y$ as a function of $x\\in\\mathbb{R}^n$ and rely on the following 3 assumptions\n",
    "- $y|x;\\ \\theta\\sim\\rm{ExpFamily}(\\eta)$\n",
    "- $h_\\theta(x) = E[y|x;\\theta]$\n",
    "- $\\eta=\\theta^Tx$\n",
    "Common exponential distributions include Bernouli, Gaussian, Poisson, Geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine (SVM)\n",
    "The goal of support vector machines is to find the line that maximizes the minimum distance of two set of points to the line.\n",
    "- optimal margin classifier:  \n",
    "$h(x)=\\rm{sign}(w^Tx-b)$, where $\\frac{1}{2}\\min||w||^2$, such that $y^{(i)}(w^Tx^{(i)}-b)\\geqslant1$\n",
    "- Hinge loss: $L(z, y)=\\max(0, 1-yz)$\n",
    "- Kernel: Given a feature mapping $\\phi$, define the kernel $K$ to be $K(x, y)=\\phi(x)^T\\phi(y)$\n",
    " - Gaussian kernel (radial basis function, RBF): $K(x, y)=\\exp(-\\displaystyle\\frac{||y-x||^2}{2\\sigma^2})$\n",
    " - Polynomial kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "- Assumption: Naive Bayes assumes that features of each data points are all independent, there fore\n",
    "$$P(x|y)=\\prod_{i=1}^n P(x_i|y)$$\n",
    "- Solution: $P(y|x_n)=\\frac{P(x|y)P(y)}{P(x_n)}=\\frac{\\prod_{i=1}^n P(x_i|y)P(y)}{P(x_n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Discriminant Analysis\n",
    "- Assumption: assume $y$ and $x|y_i$, $i=(0,1)$ are such that\n",
    " - $y\\sim\\rm{Bernoulli}(\\phi)$\n",
    " - $x|y=0\\sim\\mathcal{N}(\\mu_0,\\Sigma)$\n",
    " - $x|y=1\\sim\\mathcal{N}(\\mu_1,\\Sigma)$\n",
    "- Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest (RF)\n",
    "- bootstrapping\n",
    "- select only a random subset of features\n",
    "- limit tree depth and number of leaves\n",
    "- averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "The idea of boosting methods is to combine several weak learners to form a stronger one.\n",
    "- Adaptive boosting (Ada boosting): High weights are put on errors to improve at the next boosting step\n",
    "- Gradient boosting: Weak learners trained on remaining errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K nearest neighbors (KNN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
